\chapter{5. Evaluation of Trials and Benchmarks}\label{ch:trialevaluation}
%(JOHN)
The three key attributes of a forecast solution evaluation associated with a trial or benchmark (T/B) are (1) representativeness (2) significance and (3) relevance.  If any one of these are not satisfactorily achieved than evaluation will not provide meaningful information to the forecast solution decision process and the resources employed in the trial or benchmark will effectively have been wasted.  Unfortunately, it many not be obvious to the conductor of a T/B or the user of the information produced by the T/B whether or not these three attributes have not been achieved in the evaluation.  This section will present the issues associated with each attribute and provide guidance on how to maximize the likelihood that each will be achieved. 

\section{Representativeness} %moved tp 02_evaluation_uncertainty

Representativeness refers to the relationship between the results of a trial or benchmark evaluation and the performance that is ultimately obtained in the operational use of a forecast solution.  It essentially addresses the question of whether or not the results of the evaluation are likely to be a good predictor of the actual forecast performance that will be achieved for an operational application. These are many factors that influence the ability of the T/B evaluation results to be a good predictor of future operational performance. Four of the most crucial factors as: 
\begin{enumerate}
\item size and composition of the evaluation sample,
\item quality of the data from the forecast target sites,
\item the formulation and enforcement of rules governing the submission of T/B forecasts (sometimes referred to as “fairness”),
\item availability of a complete and consistent set of T/B information to all T/B participants (sometimes referred to as “transparency”)
\end{enumerate}

%moved to \subsection{Size of the evaluation sample} in 02_evaluation_uncertainty
The size of the evaluation sample is one of the most important representativeness factors.  The size of the sample is a key factor in determining how much noise is present in the results.  The use of a small sample increases the probability that any conclusions reached from the evaluation will be due to noise (random and unrepresentative events) in the sample.  For example, the occurrence of very unusual weather events for a few days in a short sample may dominate the evaluation results.  That leads to the question of how large of a sample is adequate?  A commonly used target sample size guideline when gathering data for statistical analysis is 30.  If all the sample points are independent then a sample of 30 provides a reasonable adequate minimization that sampling noise will impact the conclusions.  But the key phrase is that the sample data points must be independent (uncorrelated) for this guideline to be valid.  However, weather processes are typically highly correlated over time periods of 3 to 4 days. This mean that an adequate sample from a continuous evaluation period should be 3 to 4 times larger than 30 or 90 to 120 days.
The composition of an evaluation sample is another key issue.  The composition should be constructed so that all significant modes of variation of the forecast variable (e.g. wind power production) are included in the evaluation sample.  For example if there is a high wind season and a low wind season then both should have a representative number of cases in the evaluation sample.  However, if this is not practical than at least there should at least be a representative sample of the most important modes for the application (e.g. high wind season when the speeds are near cutout or periods when the wind speed is frequently in the highly sensitive steeply sloped part of the turbine power curve).
The data from the forecast target location is typically used for two purposes: (1) as training data for the statistical components of each forecast system and (2) evaluation of the forecast performance.  If the data has many quality issues then the representativeness of both applications is compromised.  
The quality issues may include: 
\begin{enumerate}
    \item out of range or locked values
    \item biased values due to issues with measurement devices or location of measurement
    \item values that are unrepresentative of meteorological conditions because of undocumented outages or curtailments
\end{enumerate}

If a substantial part of data with these issues is used is used in the B/T for either of the two purposes, the results will likely not be representative of the true skill of the forecasting solutions that are being evaluated.
The third important factor is the formulation and enforcement of rules for the T/B. This is sometimes noted as a “fairness” issue and it is indeed an issue of fairness to the forecast providers who are typically competing to demonstrate the skill of their system and thereby obtain an award of a contract for their services.  However, from the user’s perspective it is a representativeness issue.  If it is possible to for some forecasting solution providers to provide forecasts with unrepresentative skill then the conclusions of the entire evaluation process are questionable.  A couple of examples can illustrate this point.


\emph{Examples:} % moved to 05_best_practice \subsection{Performance Analysis}
\begin{enumerate}
    \item Lack of check or enforcement of forecast delivery time \\
Therefore, it would be possible for a forecast provider to deliver forecasts at a later time (perhaps overwriting a forecast that was delivered at the required time) and use later data to add skill to their forecast or even wait until the outcome for the forecast period is known.  Although one might think that such explicit cheating is not likely to occur in this type of technical evaluation, experience has indicated that it is not that uncommon if the situation enables its occurrence.
    \item Selective delivery of forecasts\\ 
This example illustrates how the results might be manipulated with explicit cheating by taking advantage of loopholes in the rules.  In this example the issue is that the B/T protocol does specify any penalty for missing a forecast delivery and the evaluation metrics are simply computed on whatever forecasts are submitted by each provider.  As a forecast provider it is easy to estimate the “difficulty” of each forecast period and to simply not deliver any forecasts during periods that are likely to be difficult and therefore prone to large errors.  This is an excellent way to improve forecast performance scores.  Of course, it makes the results unrepresentative of what is actually needed by the user.  Often it is good performance during the difficult forecast periods that are most valuable to a user.
    \item Inconsistent Information\\
The fourth key factor is the availability of a complete and consistent set of T/B information to all participants in the T/B. Incomplete or inconsistent information distribution can occur in many ways.  For example, one participant may ask a question and the reply is only provided to the participant who submitted the inquiry.   
\end{enumerate}

\section{Significance}

Significance refers to the ability to differentiate between performance differences that are due to noise (quasi-random processes) in the evaluation process and those that are due to meaningful differences in skill among forecast solutions. Performance differences that are due to noise in the T/B process are not likely to be good predictors of the performance that a user will experience in a long-term operational application of a solution.
[Show an example from Craig Collier’s experiments?]

\section{Relevance} %moved to 02_evaluation_uncertainty \section{Relevance}\label{sec:relevance}

Relevance refers to the degree of alignment between the evaluation metrics used for a T/B evaluation and the true sensitivity of a user’s application(s) to forecast error. If these two items are not well aligned then even though an evaluation process is representative and the results show significant differences among solutions, the conclusions may not be a relevant basis for selecting the best solution for the application. There are a number of issues related to the relevance factor.

\begin{enumerate} 
    \item Best Metric\\
First, the selection of the best metric may be complex and difficult.  The ideal approach is to formulate a cost function that transforms forecast error to the application-related consequences of those errors.  This could a monetary implication or it might be another type of consequence (for example a reliability metric for grid operations).  However, it is not feasible to do this, another approach is to use a matrix of performance metrics that measure a range of forecast performance attributes.
    \item Performance Attributes \\
If there a range of forecast performance attributes that are relevant to a user’s application, it most likely will not be possible to optimize a single forecast to achieve optimal performance for all of the relevant metrics. In that case, the best solution is to obtain multiple forecasts with each being optimized for a specific application and its associated metric. 
    \item Employing multiple Forecasters \\
Another type of issue arises when the user intends to employ multiple (N) forecast solutions and create a composite forecast from the information provided by each individual forecast. In this case it may be tempting to select the best N performing forecasts according to the metric or metrics identified as most relevant by the user.  However, that is not the best way to get the most relevant answer for the multiple provider scenario.  In that case the desired answer is to select the N forecasts that provide the best composite forecast.  This may not be the set of N forecasts that individually perform the best.  It is the set of forecasts that best complement each other.  For example, the two best forecasts according to a metric such as the RMSE may be highly correlated and provide essentially the same information.  In that case, a forecast solution with a higher (worse) RMSE may be less correlated with the lowest RMSE forecast and therefore be a better complement to that forecast.  
\end{enumerate}

\section{summary of Recommended Best Practices}

The conductors of a T/B should consider all of the factors noted in the three key areas for a T/B:
\begin{enumerate}
    \item representativeness
    \item significance
    \item relevance
\end{enumerate}

A useful approach is to create a evaluation plan matrix that lists all of the factors noted in the discussion in this section and how the user’s evaluation plan addresses them.
