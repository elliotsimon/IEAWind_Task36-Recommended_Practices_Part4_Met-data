\chapter{Incentive Schemes}\label{ch:incentives}
%moved to 05_best_practice \label{subsec:incentivization}

Incentive performance measures are a mehtod to incentivice a forecaster to put emphasis in the developemnt of methodologies on targets that are important for the end-user, but also a way for the end-user to monitor progress on the forecasting system. 

The difficulty, as with all verification is to decide the metrics, periods and aggregation level for the incentive. 
Another aspect to consider are the exclusion principles for curtailments or other periods, where the system was not supplied with the full available or installed capacity see also \ref{sec:filtering_process}). 
The recommendations are therefore focused on these 4 aspects:
\begin{enumerate}
    \item selection of relevant target parameter
    \item selection of relevant metrics
    \item selection of relevant verification horizons
    \item exclusion principles
\end{enumerate}


\section{Selection of relvant target Parameter}
%moved to 05_best_practice \label{subsec:improvement}

The selection process of relevant target parameters is highly dependent on the forecasting solution. The objective and proper setup of verification as well as evaluatio metrics and frameworks can be found in \ref{ch:metrics}, \ref{ch:eval_of_typ}, \ref{ch:eval_of_oper}, \ref{ch:trialevaluation} and section \ref{sec:evaluation_framework}. 
The following recommendation is solely on the type of benchmarks that may be applied. \\

\textbf{\emph{Recommendation}}: Selection of relevant target parameters need to be defined to provide a focus area for the forecaster. Comparison to a previous period, to a persistence or a set target that is realistic can circumvent a number of constraints that are difficult to exclude in an evaluation. 

\begin{table}[h!]
 \caption{List of possible benchmark types for an incentive scheme. The types are not meant to be stand-alone and may also be combined to hybrids.}
  \label{table:incentive}
\centering
 \begin{tabular}{ |m{8em}||m{10cm}|  }
 \hline
 & \\
 \textbf{Benchmark} & \textbf{Comment/Recommendation} \\
 & \\
 \hline\hline
Improvement over persistence & Comparison against persistence is the same as comparing “not having a forecast” to having one. Useful measure for short-term forecasts as a mean of evaluating the improvement of applying forecast information to measurements. 
Note: be aware of data quality issues when evaluating, especially in the case of constant values that benefit persistance, while the forecast provides a realistic view.\\
\hline
Improvement over past evaluation period / forecast & If improvement is important, the comparison to a past evaluation can be useful, especially in long-term contracts. In this way, the forecaster is forced to continue to improve and the target is moved with the improvements. The payment structure however needs to reflect that improvements reduce over time and have an upper limit. \\
\hline
Comparison against set targets & If the required performance of a forecasting system can be defined, clear targets should be set and the payment directed according to a percentage from 0-100\% of the achieved target.  \\
\hline
Categorised error evaluation & An effective evaluation form is to not set one error target, but categorise errors instead e.g. large, medium and small errors. If large errors pose a critical issue, then improvement on these may be incentivized higher and vice versa. 
The end-user can in that way steer the development and focus of improvements. \\
 \hline
 \end{tabular}
\end{table}


\section{Selection of relevant Metrics}



\section{Selection of relevant Verification Horizons}


\section{Exclusion Principles}


\section{Summary of Recommendations} 
